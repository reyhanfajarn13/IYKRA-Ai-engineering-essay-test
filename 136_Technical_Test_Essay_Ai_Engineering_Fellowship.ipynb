{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 1: Data Ingestion**"
      ],
      "metadata": {
        "id": "86v_golPP2iy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Using Google Cloud Storage to store raw data from multiple sources**"
      ],
      "metadata": {
        "id": "w9HTA6XqQFdt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "from google.cloud import bigquery\n",
        "\n",
        "def upload_to_gcs(bucket_name, file_name, data):\n",
        "    client = storage.Client()\n",
        "    bucket = client.get_bucket(bucket_name)\n",
        "    blob = bucket.blob(file_name)\n",
        "    blob.upload_from_string(data)\n",
        "    print(f\"File {file_name} uploaded to {bucket_name}\")"
      ],
      "metadata": {
        "id": "elV_2jPZQDdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ecommerce_data = \"ecommerce_sales.csv\"\n",
        "social_media_data = \"social_trends.json\"\n",
        "bucket_name = \"fashion-data-lake\"\n",
        "upload_to_gcs(bucket_name, ecommerce_data, ecommerce_data)\n",
        "upload_to_gcs(bucket_name, social_media_data, social_media_data)"
      ],
      "metadata": {
        "id": "UdKBvhF2QUAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 2: Data Preprocessing**"
      ],
      "metadata": {
        "id": "2ZaJ8DcMQPFf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Using BigQuery to preprocess structured data, clean missing values, and normalize**"
      ],
      "metadata": {
        "id": "aByENHKTQf7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data():\n",
        "    client = bigquery.Client()\n",
        "    query = \"\"\"\n",
        "        SELECT\n",
        "            product_id,\n",
        "            sales,\n",
        "            IFNULL(social_trends_score, 0) as trend_score,\n",
        "            (sales - MIN(sales) OVER()) / (MAX(sales) OVER() - MIN(sales) OVER()) as normalized_sales\n",
        "        FROM\n",
        "            `project.dataset.sales_data`\n",
        "        WHERE\n",
        "            sales IS NOT NULL\n",
        "    \"\"\"\n",
        "    job = client.query(query)\n",
        "    result = job.result()  # Waits for query to finish\n",
        "    return result\n",
        "\n",
        "processed_data = preprocess_data()"
      ],
      "metadata": {
        "id": "VQaFv7CBQlP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 3: GAN Training with TensorFlow**"
      ],
      "metadata": {
        "id": "31Klw0z5Qqo_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# GAN Generator Model\n",
        "def build_generator():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Dense(256, input_dim=100)) #please fill in the empty value\n",
        "    model.add(layers.LeakyReLU(0.2))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dense(512)) #please fill in the empty value\n",
        "    model.add(layers.LeakyReLU(0.2)) #please fill in the empty value\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dense(1024)) #please fill in the empty value\n",
        "    model.add(layers.LeakyReLU(0.2))#please fill in the empty value\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dense(784, activation='tanh'))  # Fashion design as 28x28 image\n",
        "    return model\n",
        "\n",
        "# GAN Discriminator Model\n",
        "def build_discriminator():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Dense(512, input_dim=784)) #please fill in the empty value\n",
        "    model.add(layers.LeakyReLU(0.2))\n",
        "    model.add(layers.Dense(256))\n",
        "    model.add(layers.LeakyReLU(0.2))\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "    return model\n",
        "\n",
        "# Compile GAN\n",
        "def compile_gan(generator, discriminator):\n",
        "    discriminator.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    discriminator.trainable = False\n",
        "    gan_input = tf.keras.Input(shape=(100,)) #please fill in the empty value\n",
        "    generated_image = generator(gan_input)\n",
        "    gan_output = discriminator(generated_image)\n",
        "    gan = tf.keras.Model(gan_input, gan_output)\n",
        "    gan.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "    return gan\n",
        "\n",
        "# Train GAN\n",
        "def train_gan(generator, discriminator, gan, epochs=1000, batch_size=128):\n",
        "    for epoch in range(epochs):\n",
        "        noise = tf.random.normal([batch_size, 100])\n",
        "        generated_images = generator.predict(noise)\n",
        "        real_images = processed_data.sample(batch_size)\n",
        "        labels_real = tf.ones((batch_size,1)) #please fill in the empty value\n",
        "        labels_fake = tf.zeros((batch_size,1)) #please fill in the empty value\n",
        "\n",
        "        d_loss_real = discriminator.train_on_batch(real_images, labels_real) #please fill in the empty value\n",
        "        d_loss_fake = discriminator.train_on_batch(generator_images, labels_fake) #please fill in the empty value\n",
        "\n",
        "        noise = tf.random.normal([batch_size, 100])\n",
        "        g_loss = gan.train_on_batch(noise, tf.ones((batch_size, 1)))\n",
        "\n",
        "        if epoch % 100 == 0:\n",
        "            print(f\"Epoch {epoch}, D Loss Real: {d_loss_real}, D Loss Fake: {d_loss_fake}, G Loss: {g_loss}\")"
      ],
      "metadata": {
        "id": "1DdnVr9YQwRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Build and train the GAN**"
      ],
      "metadata": {
        "id": "10TUqVugQ2SC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator = build_generator()\n",
        "discriminator = build_discriminator()\n",
        "gan = compile_gan() #please fill in the empty value\n",
        "train_gan(generator, discriminator, gan)"
      ],
      "metadata": {
        "id": "-HyiUs4QQ6_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 4: Model Deployment using Vertex AI**"
      ],
      "metadata": {
        "id": "3-MMW7boQ9yh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import aiplatform\n",
        "\n",
        "def deploy_model_to_vertex_ai(model_artifact, endpoint_name):\n",
        "    aiplatform.init()\n",
        "    model = aiplatform.Model.upload(display_name=\"fashion-gan-model\", artifact_uri=model_artifact)\n",
        "    endpoint = model.deploy(machine_type=\"n1-standard-4\", endpoint_name=endpoint_name)\n",
        "    return endpoint\n",
        "\n",
        "endpoint = deploy_model_to_vertex_ai(\"gs://fashion-model-bucket/gan_model\", \"fashion-gan-endpoint\")\n",
        "\n",
        "# Step 5: Real-Time Inference\n",
        "import numpy as np\n",
        "\n",
        "def generate_fashion_design():\n",
        "    noise = np.random.normal(0, 1, (1, 100))\n",
        "    design = generator.predict(noise)\n",
        "    print(f\"Generated fashion design: {design}\")\n",
        "    return design\n",
        "\n",
        "generate_fashion_design()"
      ],
      "metadata": {
        "id": "-1sEU6fbRHns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Kubernetes Auto-Scaling**"
      ],
      "metadata": {
        "id": "vRTwVmO2RK_0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Please create a simple Kubernetes YAML deployment for auto-scaling the GAN inference system and expose to 8080**"
      ],
      "metadata": {
        "id": "PlNoEvpGRO-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "apiVersion: apps/v1\n",
        "kind: Deployment\n",
        "metadata:\n",
        "  name: gan-ai-inference\n",
        "spec:\n",
        "  replicas: 2\n",
        "  selector:\n",
        "    matchLabels:\n",
        "      app: gan-ai-inference\n",
        "  template:\n",
        "    metadata:\n",
        "      labels:\n",
        "        app: gan-ai-inference\n",
        "    spec:\n",
        "      containers:\n",
        "        - name: gan-ai-container\n",
        "          image: gan-ai-image:latest\n",
        "          ports:\n",
        "            - containerPort: 8080\n",
        "---\n",
        "apiVersion: v1\n",
        "kind: Service\n",
        "metadata:\n",
        "  name: gan-ai-service\n",
        "spec:\n",
        "  type: LoadBalancer\n",
        "  ports:\n",
        "    - port: 8080\n",
        "      targetPort: 8080\n",
        "  selector:\n",
        "    app: gan-ai-inference\n",
        "---\n",
        "apiVersion: autoscaling/v1\n",
        "kind: HorizontalPodAutoscaler\n",
        "metadata:\n",
        "  name: gan-ai-hpa\n",
        "spec:\n",
        "  scaleTargetRef:\n",
        "    apiVersion: apps/v1\n",
        "    kind: Deployment\n",
        "    name: gan-ai-inference\n",
        "  minReplicas: 1\n",
        "  maxReplicas: 5\n",
        "  targetCPUUtilizationPercentage: 70\n"
      ],
      "metadata": {
        "id": "1TRInVX1XKBz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}